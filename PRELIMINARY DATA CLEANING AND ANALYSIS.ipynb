{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1272d3d5",
   "metadata": {},
   "source": [
    "### PRELIMINARY DATA CLEANING AND ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8dca9b",
   "metadata": {},
   "source": [
    "### Option 1 preferably better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91346e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load SpaCy Dutch model\n",
    "nlp = spacy.load('nl_core_news_lg')\n",
    "\n",
    "# Define a function for additional cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\W+', ' ', text)      # Remove all non-word characters\n",
    "    text = re.sub(r'[0-9]', '', text)     # Remove digits\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # Remove single characters\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', ' ', text)  # Remove dates\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)  # Remove standalone numbers\n",
    "    text = re.sub(r'\\b00\\b', ' ', text)   # Remove double zeroes\n",
    "    text = re.sub(r'(?<!\\S)\\d+(?=-[a-zA-Z])', '', text)  # Keep numbers like '24-jarige'\n",
    "    return text\n",
    "\n",
    "# Tokenization, stopword removal, and lemmatization\n",
    "def preprocess_content(text):\n",
    "    text = clean_text(text)  # Clean text\n",
    "    tokens = word_tokenize(text, language='dutch')  # Tokenize\n",
    "    tokens = [token for token in tokens if token not in set(stopwords.words('dutch'))]  # Remove stopwords\n",
    "    lemmas = [token.lemma_.lower() for token in nlp(' '.join(tokens))]  # Lemmatize and lowercase\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'C:/Users/xx/Downloads/Artikelen_Sanders.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply the preprocessing function to the 'content' column\n",
    "df['processed_content'] = df['content'].astype(str).apply(preprocess_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b306e",
   "metadata": {},
   "source": [
    "### Option 2 Using tokenizer from BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning using tokenizer from BERTje\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load NLTK resources - Dutch stopwords and SpaCy Dutch model\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "dutch_stop_words = set(stopwords.words('dutch'))\n",
    "additional_stop_words = ['één', 'kommen', 'zeggen','zullen','moeten','gaan', 'wij']  # Add your stop words here\n",
    "\n",
    "# Add the additional stop words to the Dutch stop words set\n",
    "dutch_stop_words.update(additional_stop_words)\n",
    "\n",
    "# Load SpaCy Dutch model\n",
    "nlp = spacy.load('nl_core_news_lg')\n",
    "\n",
    "# Initialize the tokenizer for BERTje\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/xx/Downloads/200_random_artikelen.csv')  # Replace with your file path\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to string and format text\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\r\\n\", \" \").strip()\n",
    "\n",
    "    # Basic cleaning\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    \n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove all non-word characters\n",
    "    \n",
    "    # Replace punctuation with space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Optional: Remove digits\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    \n",
    "       \n",
    "    # Remove dates (formats like 12-12-2000 or 12/12/2000)\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', ' ', text)\n",
    "    \n",
    "    # Remove standalone numbers and double zeroes ('00')\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    text = re.sub(r'\\b00\\b', ' ', text)\n",
    "\n",
    "    # Replace punctuation with space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Optional: Remove single characters - might remove valid single-letter words (e.g., 'a', 'I' in English)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    \n",
    "\n",
    "    # Lemmatization with SpaCy\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "    # Removing stopwords\n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in dutch_stop_words]\n",
    "\n",
    "    # Rejoin lemmatized words\n",
    "    cleaned_text = ' '.join(lemmas)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Applying preprocessing to your text column\n",
    "df['cleaned_text'] = df['content'].apply(preprocess_text)  # Replace 'content' with the actual column name in your dataset\n",
    "\n",
    "# Applying the tokenizer\n",
    "df['tokenized_text'] = df['cleaned_text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# The resulting 'tokenized_text' column will have the text data tokenized "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c274c",
   "metadata": {},
   "source": [
    "### EXTRACT THE MOST COMMON WORDS IN PROCESSED CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the length of the text in 'content' is less than 200 symbols RAW DATA\n",
    "short_text_rows = df[df['content'].str.len() < 200]\n",
    "\n",
    "# Print these rows along with the text\n",
    "for index, row in short_text_rows.iterrows():\n",
    "    print(f\"Row {index}:\")\n",
    "    print(row['content'])\n",
    "    print(\"---\")  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a04a3",
   "metadata": {},
   "source": [
    "<strong>FILTER THE ARTICLES WITH LESS THAN 200 SYMBOLS FROM CONTENT</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce533441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the length of the text in 'content' is less than 200 symbols PROCESSED DATA\n",
    "short_text_rows = df[df['processed_content'].str.len() < 200]\n",
    "\n",
    "# Print these rows along with the text\n",
    "for index, row in short_text_rows.iterrows():\n",
    "    print(f\"Row {index}:\")\n",
    "    print(row['processed_content'])\n",
    "    print(\"---\")  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming 'df' is your DataFrame loaded from 'Artikelen_Sanders.csv'\n",
    "\n",
    "# Function to find special characters and count their occurrences RAW DATA\n",
    "def find_and_count_special_characters(text):\n",
    "    # Define a regular expression pattern for characters outside of the basic Latin set\n",
    "    pattern = r'[^\\w\\s,.?!;:\\'\"-]'\n",
    "    # Find all matches\n",
    "    special_chars = re.findall(pattern, text)\n",
    "    # Count occurrences of each unique character\n",
    "    char_count = {char: special_chars.count(char) for char in set(special_chars)}\n",
    "    return char_count\n",
    "\n",
    "# Apply the function to each row in your DataFrame\n",
    "df['special_characters_count'] = df['content'].apply(find_and_count_special_characters)\n",
    "\n",
    "# To print the type of symbol and its occurrence per row in descending order,\n",
    "# you can iterate over each row and print the information.\n",
    "for index, row in df.iterrows():\n",
    "    # Check if there are special characters in the row\n",
    "    if row['special_characters_count']:\n",
    "        # Sort the dictionary by value in descending order\n",
    "        sorted_chars = dict(sorted(row['special_characters_count'].items(), key=lambda item: item[1], reverse=True))\n",
    "        print(f\"Row {index}: {sorted_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8033fa",
   "metadata": {},
   "source": [
    "<strong>Function to find special characters and count their occurrences</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe798d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming 'df' is your DataFrame loaded from 'Artikelen_Sanders.csv'\n",
    "\n",
    "# Function to find special characters and count their occurrences\n",
    "def find_and_count_special_characters(text):\n",
    "    # Define a regular expression pattern for characters outside of the basic Latin set\n",
    "    pattern = r'[^\\w\\s,.?!;:\\'\"-]'\n",
    "    # Find all matches\n",
    "    special_chars = re.findall(pattern, text)\n",
    "    # Count occurrences of each unique character\n",
    "    char_count = {char: special_chars.count(char) for char in set(special_chars)}\n",
    "    return char_count\n",
    "\n",
    "# Apply the function to each row in your DataFrame\n",
    "df['special_characters_count'] = df['content'].apply(find_and_count_special_characters)\n",
    "\n",
    "# To print the type of symbol and its occurrence per row in descending order,\n",
    "# you can iterate over each row and print the information.\n",
    "for index, row in df.iterrows():\n",
    "    # Check if there are special characters in the row\n",
    "    if row['special_characters_count']:\n",
    "        # Sort the dictionary by value in descending order\n",
    "        sorted_chars = dict(sorted(row['special_characters_count'].items(), key=lambda item: item[1], reverse=True))\n",
    "        print(f\"Row {index}: {sorted_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming 'df' is your DataFrame loaded from 'Artikelen_Sanders.csv'\n",
    "\n",
    "# Function to find special characters and count their occurrences PROCESSED DATA\n",
    "def find_and_count_special_characters(text):\n",
    "    # Define a regular expression pattern for characters outside of the basic Latin set\n",
    "    pattern = r'[^\\w\\s,.?!;:\\'\"-]'\n",
    "    # Find all matches\n",
    "    special_chars = re.findall(pattern, text)\n",
    "    # Count occurrences of each unique character\n",
    "    char_count = {char: special_chars.count(char) for char in set(special_chars)}\n",
    "    return char_count\n",
    "\n",
    "# Apply the function to each row in your DataFrame\n",
    "df['special_characters_count'] = df['processed_content'].apply(find_and_count_special_characters)\n",
    "\n",
    "# To print the type of symbol and its occurrence per row in descending order,\n",
    "# you can iterate over each row and print the information.\n",
    "for index, row in df.iterrows():\n",
    "    # Check if there are special characters in the row\n",
    "    if row['special_characters_count']:\n",
    "        # Sort the dictionary by value in descending order\n",
    "        sorted_chars = dict(sorted(row['special_characters_count'].items(), key=lambda item: item[1], reverse=True))\n",
    "        print(f\"Row {index}: {sorted_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741a80b",
   "metadata": {},
   "source": [
    "<strong>Function to count symbols in a string</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee99937",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Assuming your DataFrame is already loaded as 'data'\n",
    "# data = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Function to count symbols in a string\n",
    "def count_symbols(text):\n",
    "    return len(text)\n",
    "\n",
    "# Count symbols in 'content' and 'processed_content'\n",
    "df['content_symbol_count'] = df['content'].apply(count_symbols)\n",
    "df['processed_content_symbol_count'] = df['processed_content'].apply(count_symbols)\n",
    "\n",
    "# Select only the relevant columns for display\n",
    "columns_to_display = ['content', 'processed_content', 'content_symbol_count', 'processed_content_symbol_count']\n",
    "\n",
    "# Display the table with 'content', 'processed_content', and their symbol counts\n",
    "print(df[columns_to_display])  # You can adjust the number of rows displayed by changing 'head()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637905f7",
   "metadata": {},
   "source": [
    "<strong>Find capitalized words </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_capitalized_words(text):\n",
    "    # Regular expression to find words starting with a capital letter\n",
    "    pattern = r'\\b[A-Z][a-z]*\\b'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Iterate through each row and print capitalized words\n",
    "for index, row in df.iterrows():\n",
    "    capitalized_words = find_capitalized_words(row['processed_content'])\n",
    "    print(f\"Row {index} capitalized words: {capitalized_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6f7ea",
   "metadata": {},
   "source": [
    "<strong>Convert 'Datum' to datetime</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c98fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'datum' to datetime\n",
    "df['Datum'] = pd.to_datetime(df['Datum'], errors='coerce')  # 'coerce' will turn unparseable strings into NaT\n",
    "\n",
    "# Remove rows where 'datum' is in 1990\n",
    "# This filters the DataFrame to only include rows where the year is not 1990\n",
    "df = df[df['Datum'].dt.year != 1990]\n",
    "\n",
    "# Save the filtered DataFrame back to CSV or continue with further processing\n",
    "#df.to_csv('filtered_dataset.csv', index=False)  # Save to a new file or overwrite by using file_path\n",
    "\n",
    "#print(\"Rows with 'datum' in 1990 have been removed and the dataset is saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3985a90",
   "metadata": {},
   "source": [
    "<strong>EXPRORE THE PROCESSED_CONTENT COLUMN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the DataFrame and print each 'processed_content'\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print(f\"Row {index}: {row['processed_content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423203ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index of the row you want to print RAW DATA\n",
    "row_index = 42  # Replace with the index of the row you're interested in\n",
    "\n",
    "# Access and print the text of the specified row\n",
    "if row_index < len(data):\n",
    "    print(f\"Text in row {row_index}:\")\n",
    "    print(data.loc[row_index, 'content'])\n",
    "else:\n",
    "    print(f\"Row index {row_index} is out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fe429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index of the row you want to print - PROCESSED\n",
    "row_index = 42  # Replace with the index of the row you're interested in\n",
    "\n",
    "# Access and print the text of the specified row\n",
    "if row_index < len(data):\n",
    "    print(f\"Text in row {row_index}:\")\n",
    "    print(data.loc[row_index, 'processed_content'])\n",
    "else:\n",
    "    print(f\"Row index {row_index} is out of range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d607db",
   "metadata": {},
   "source": [
    "<strong>The OCR mistakes which cause misinterpretation of 'Content'</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef394915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index of the row you want to print RAW DATA\n",
    "row_index = 632  # Replace with the index of the row you're interested in\n",
    "\n",
    "# Access and print the text of the specified row\n",
    "if row_index < len(data):\n",
    "    print(f\"Text in row {row_index}:\")\n",
    "    print(data.loc[row_index, 'content'])\n",
    "else:\n",
    "    print(f\"Row index {row_index} is out of range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddf09b",
   "metadata": {},
   "source": [
    "<strong>AN ARTICLE TEXT WITH EXAMPLE OF OCR MISTAKES</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea00052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index of the row you want to print RAW DATA\n",
    "row_index = 346  # Replace with the index of the row you're interested in\n",
    "\n",
    "# Access and print the text of the specified row\n",
    "if row_index < len(data):\n",
    "    print(f\"Text in row {row_index}:\")\n",
    "    print(data.loc[row_index, 'content'])\n",
    "else:\n",
    "    print(f\"Row index {row_index} is out of range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659921",
   "metadata": {},
   "source": [
    "## Supershort news item \n",
    "Since the numbers have been cleaned it is crucial for us to understand whether we need to change the cleaning conditions or whether such constructs are not as important for our research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index of the row you want to print RAW DATA\n",
    "row_index = 295  # Replace with the index of the row you're interested in\n",
    "\n",
    "# Access and print the text of the specified row\n",
    "if row_index < len(data):\n",
    "    print(f\"Text in row {row_index}:\")\n",
    "    print(data.loc[row_index, 'content'])\n",
    "else:\n",
    "    print(f\"Row index {row_index} is out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index of the row you want to print RAW DATA\n",
    "row_index = 40  # Replace with the index of the row you're interested in\n",
    "\n",
    "# Access and print the text of the specified row\n",
    "if row_index < len(data):\n",
    "    print(f\"Text in row {row_index}:\")\n",
    "    print(data.loc[row_index, 'content'])\n",
    "else:\n",
    "    print(f\"Row index {row_index} is out of range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed7bac",
   "metadata": {},
   "source": [
    "## Symbol count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Function to count symbols in each row\n",
    "def count_symbols(text):\n",
    "    return len(text)\n",
    "\n",
    "# Apply the function to the 'processed_content' column\n",
    "data['symbol_count'] = data['processed_content'].apply(count_symbols)\n",
    "\n",
    "# Print the symbol counts for each row\n",
    "for index, row in data.iterrows():\n",
    "    print(f\"Row {index} has {row['symbol_count']} symbols in processed_content.\")\n",
    "\n",
    "# Now data DataFrame includes the 'symbol_count' column\n",
    "# You can save this DataFrame if needed\n",
    "# data.to_csv('your_dataset_with_symbol_count.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9ae78",
   "metadata": {},
   "source": [
    "<strong>FILTER THE ROWS WITH LESS THAN 200 SYMBOLS IN PROCESSED_CONTENT</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'symbol_count' is less than 200 in Processed_content\n",
    "filtered_data = data[data['symbol_count'] < 200]\n",
    "\n",
    "# Print the 'processed_content' of these rows\n",
    "print(\"Processed Content with Less Than 200 Symbols:\")\n",
    "for index, row in filtered_data.iterrows():\n",
    "    print(f\"Row {index}: {row['processed_content']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388cdb02",
   "metadata": {},
   "source": [
    " ###  <strong>Capitalized letters check after cleaning and preprocessing</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data\n",
    "\n",
    "def find_capitalized_words(text):\n",
    "    # Regular expression to find words starting with a capital letter\n",
    "    pattern = r'\\b[A-Z][a-z]*\\b'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Iterate through each row and print capitalized words\n",
    "for index, row in df.iterrows():\n",
    "    capitalized_words = find_capitalized_words(row['processed_content'])\n",
    "    print(f\"Row {index} capitalized words: {capitalized_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6120198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_capitalized_and_special_words(text):\n",
    "    # Regular expression for words starting with a capital letter\n",
    "    start_cap_pattern = r'\\b[A-Z][a-z]*\\b'\n",
    "    \n",
    "    # Pattern for words with internal capital letters (camelCase)\n",
    "    camel_case_pattern = r'\\b[a-z]+[A-Z][a-zA-Z]*\\b'\n",
    "    \n",
    "    # Pattern for all-uppercase words\n",
    "    uppercase_pattern = r'\\b[A-Z]+\\b'\n",
    "\n",
    "    # Finding matches\n",
    "    start_cap_words = re.findall(start_cap_pattern, text)\n",
    "    camel_case_words = re.findall(camel_case_pattern, text)\n",
    "    uppercase_words = re.findall(uppercase_pattern, text)\n",
    "\n",
    "    return start_cap_words, camel_case_words, uppercase_words\n",
    "\n",
    "# Apply the function to the 'processed_content' column\n",
    "data[['start_cap_words', 'camel_case_words', 'uppercase_words']] = data['processed_content'].apply(\n",
    "    lambda x: pd.Series(find_capitalized_and_special_words(x))\n",
    ")\n",
    "\n",
    "# Display the DataFrame to see the results\n",
    "print(data[['processed_content', 'start_cap_words', 'camel_case_words', 'uppercase_words']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98951fe2",
   "metadata": {},
   "source": [
    "### DEFINE KEY WORDS IN PROCESSED CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d72eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the key words in 'processed_content' and print the filtered rows and their numbers\n",
    "# Load the DataFrame\n",
    "df = data\n",
    "\n",
    "# Define the list of words you're interested in\n",
    "words_of_interest = ['arbeid', 'turkse', 'turk', 'marokkaanse', 'marokkaans', 'vrouw', 'meisje', 'dame']\n",
    "\n",
    "# Function to check if any word of interest is in the content\n",
    "def contains_word_of_interest(processed_content):\n",
    "    return any(word in processed_content for word in words_of_interest)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[df['processed_content'].apply(contains_word_of_interest)]\n",
    "\n",
    "# Print the 'content' of these filtered rows along with their row number\n",
    "for index, processed_content in filtered_df['processed_content'].iteritems():\n",
    "    print(f\"Row {index}: {processed_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77fc71c",
   "metadata": {},
   "source": [
    "<strong>Preprocess 'Titel' column</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95909c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 'Titel' column + save the cleaned_dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = data\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zäöüß]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Titel' column\n",
    "df['Titel'] = df['Titel'].astype(str).apply(clean_text)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "df.to_csv('cleaned_newspaper_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f9ade",
   "metadata": {},
   "source": [
    "<strong>DEFINE KEY WORDS IN 'TITEL' COLUMN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the DataFrame\n",
    "data = df\n",
    "\n",
    "# Define the list of words you're interested in\n",
    "words_of_interest = ['arbeid', 'turkse', 'turk','marokkaanse', 'marokkaans','vrouw', 'meisje', 'dame']  \n",
    "\n",
    "# Function to check if any word of interest is in the content\n",
    "def contains_word_of_interest(Titel):\n",
    "    return any(word in Titel for word in words_of_interest)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = data[data['Titel'].apply(contains_word_of_interest)]\n",
    "\n",
    "# Print the 'content' of these filtered rows along with their row number\n",
    "for index, Titel in filtered_df['Titel'].iteritems():\n",
    "    print(f\"Row {index}: {Titel}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1689e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataframe\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Define the words of interest\n",
    "words = ['turkse ', 'marokkanse', 'vrouw']\n",
    "\n",
    "# Initialize a dictionary to hold word counts\n",
    "word_counts = {word: {'content': 0, 'processed_content': 0} for word in words}\n",
    "\n",
    "# Count the occurrences in both columns\n",
    "for word in words:\n",
    "    word_counts[word]['content'] = df['content'].str.count(word).sum()\n",
    "    word_counts[word]['processed_content'] = df['processed_content'].str.count(word).sum()\n",
    "\n",
    "# Calculate total word count in both columns\n",
    "total_words_content = sum(df['content'].str.split().apply(len))\n",
    "total_words_processed_content = sum(df['processed_content'].str.split().apply(len))\n",
    "\n",
    "# Calculate the percentage\n",
    "word_percentages = {word: {'content': (count['content'] / total_words_content) * 100,\n",
    "                           'processed_content': (count['processed_content'] / total_words_processed_content) * 100}\n",
    "                    for word, count in word_counts.items()}\n",
    "\n",
    "# Print the results\n",
    "for word, percentages in word_percentages.items():\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Percentage in 'content': {percentages['content']:.2f}%\")\n",
    "    print(f\"Percentage in 'processed_content': {percentages['processed_content']:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6723bb",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4731e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.text import Text\n",
    "\n",
    "# Load your dataframe\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Define the words of interest\n",
    "words_of_interest = ['arbeid', 'man', 'vrouw']\n",
    "\n",
    "# Assuming 'processed_content' contains preprocessed and tokenized data\n",
    "# Convert each row of 'processed_content' into a list of words\n",
    "tokenized_contents = [content.split() for content in df['processed_content']]\n",
    "\n",
    "# Flatten the list of tokenized contents\n",
    "all_tokens = [token for content in tokenized_contents for token in content]\n",
    "\n",
    "# Create NLTK text object\n",
    "nltk_text = Text(all_tokens)\n",
    "\n",
    "# Define the width of the context window\n",
    "# variable controls how many words are shown around the target word. You can adjust this to see more or less context.\n",
    "context_window = 10  # Adjust as needed \n",
    "\n",
    "# Function to print concordance for each word of interest\n",
    "# The lines parameter in the concordance method determines how many examples are shown for each word. \n",
    "# You can increase or decrease this number based on your needs.\n",
    "\n",
    "def print_concordance(word):\n",
    "    print(f\"Concordance for '{word}':\")\n",
    "    nltk_text.concordance(word, width=context_window*2, lines=15)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print concordance for each word\n",
    "for word in words_of_interest:\n",
    "    print_concordance(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e389475",
   "metadata": {},
   "source": [
    "### Option 2 Using tokenizer from BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning using tokenizer from BERTje\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load NLTK resources - Dutch stopwords and SpaCy Dutch model\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "dutch_stop_words = set(stopwords.words('dutch'))\n",
    "additional_stop_words = ['één', 'kommen', 'zeggen','zullen','moeten','gaan', 'wij']  # Add your stop words here\n",
    "\n",
    "# Add the additional stop words to the Dutch stop words set\n",
    "dutch_stop_words.update(additional_stop_words)\n",
    "\n",
    "# Load SpaCy Dutch model\n",
    "nlp = spacy.load('nl_core_news_lg')\n",
    "\n",
    "# Initialize the tokenizer for BERTje\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/xx/Downloads/200_random_artikelen.csv')  # Replace with your file path\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to string and format text\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\r\\n\", \" \").strip()\n",
    "\n",
    "    # Basic cleaning\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    \n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove all non-word characters\n",
    "    \n",
    "    # Replace punctuation with space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Optional: Remove digits\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    \n",
    "       \n",
    "    # Remove dates (formats like 12-12-2000 or 12/12/2000)\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', ' ', text)\n",
    "    \n",
    "    # Remove standalone numbers and double zeroes ('00')\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    text = re.sub(r'\\b00\\b', ' ', text)\n",
    "\n",
    "    # Replace punctuation with space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Optional: Remove single characters - might remove valid single-letter words (e.g., 'a', 'I' in English)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    \n",
    "\n",
    "    # Lemmatization with SpaCy\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "    # Removing stopwords\n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in dutch_stop_words]\n",
    "\n",
    "    # Rejoin lemmatized words\n",
    "    cleaned_text = ' '.join(lemmas)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Applying preprocessing to your text column\n",
    "df['cleaned_text'] = df['content'].apply(preprocess_text)  # Replace 'content' with the actual column name in your dataset\n",
    "\n",
    "# Applying the tokenizer\n",
    "df['tokenized_text'] = df['cleaned_text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# The resulting 'tokenized_text' column will have the text data tokenized "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2894f9",
   "metadata": {},
   "source": [
    " ### Drop the 'Unnamed: 0' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Unnamed: 0' column which is likely the old index from the CSV file\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Check if the 'content' column exists and filter the DataFrame\n",
    "if 'content' in df.columns:\n",
    "    df = df[df['content'].apply(lambda x: len(str(x)) >= 200)]\n",
    "\n",
    "# Reset the index of the filtered DataFrame\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the filtered and reset DataFrame\n",
    "cleaned_dataset_path = \"C:/Users/xx/Downloads/200_random_artikelen_filtered.csv\"  # Replace with your desired output path\n",
    "df.to_csv(cleaned_dataset_path, index=False)\n",
    "\n",
    "# Optional: Print the DataFrame to verify the changes\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542e88d",
   "metadata": {},
   "source": [
    "### Count the number of symbols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of symbols in each row of the 'content' column\n",
    "df['Symbol_Count'] = df['cleaned_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Display the results\n",
    "print(df[['cleaned_text', 'Symbol_Count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2a8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e9f0eeb",
   "metadata": {},
   "source": [
    "### Option 3 Using retokenization and calculating the percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy Dutch model\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "\n",
    "# Function to clean text using NLTK and spaCy for Dutch\n",
    "def clean_text(text):\n",
    "    # Normalize text: lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    \n",
    "    # Tokenize with spaCy\n",
    "    doc = nlp(text)\n",
    "    spacy_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    # Tokenize with NLTK\n",
    "    nltk_tokens = nltk.word_tokenize(text)\n",
    "    dutch_stopwords = stopwords.words('dutch')\n",
    "    nltk_tokens = [word for word in nltk_tokens if word not in dutch_stopwords and word.isalnum()]\n",
    "    \n",
    "    # Combine, deduplicate tokens, and ensure no broken words\n",
    "    combined_tokens = retokenize(spacy_tokens + nltk_tokens)\n",
    "    \n",
    "    # Recompose text from tokens without spaces\n",
    "    cleaned_text = ''.join(combined_tokens)\n",
    "    \n",
    "    return cleaned_text, len(spacy_tokens), len(nltk_tokens)\n",
    "\n",
    "# Function to recompose the words, remove the spaces, and put tokens in lower letters\n",
    "def retokenize(tokenized_text):\n",
    "    tokenized = []\n",
    "    is_broken_word = False\n",
    "    temp_word = \"\"\n",
    "    for token in tokenized_text:\n",
    "        token_lower = token.lower()\n",
    "        if len(token) == 1:\n",
    "            if not is_broken_word:\n",
    "                is_broken_word = True\n",
    "                temp_word = token_lower\n",
    "            else:\n",
    "                temp_word += token_lower\n",
    "        else:\n",
    "            if is_broken_word:\n",
    "                tokenized.append(temp_word)\n",
    "                temp_word = \"\"\n",
    "                is_broken_word = False\n",
    "            tokenized.append(token_lower)\n",
    "    if is_broken_word:\n",
    "        tokenized.append(temp_word)\n",
    "    return tokenized\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv('C:/Users/xx/Downloads/Artikelen_Sanders.csv')\n",
    "\n",
    "# Apply cleaning function\n",
    "results = df['content'].apply(lambda x: clean_text(x))\n",
    "df['processed_content'] = results.apply(lambda x: x[0])\n",
    "df['spacy_word_count'] = results.apply(lambda x: x[1])\n",
    "df['nltk_word_count'] = results.apply(lambda x: x[2])\n",
    "\n",
    "# Calculate percentages\n",
    "df['spacy_percentage'] = df['spacy_word_count'] / (df['spacy_word_count'] + df['nltk_word_count']) * 100\n",
    "df['nltk_percentage'] = df['nltk_word_count'] / (df['spacy_word_count'] + df['nltk_word_count']) * 100\n",
    "\n",
    "# Save back to CSV\n",
    "df.to_csv('W_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e1142",
   "metadata": {},
   "source": [
    "### Check the processed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the processed dataset\n",
    "#df = pd.read_csv('your_dataset_processed.csv')\n",
    "\n",
    "# Access and print the 'processed_content' of row number 5 - insert the corresponding row\n",
    "# Remember that Python uses 0-based indexing, so row number 5 is actually at index 4\n",
    "row_content = df['processed_content'].iloc[4]\n",
    "\n",
    "print(row_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f679680",
   "metadata": {},
   "source": [
    "### Further steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62059099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the Dutch language model\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "\n",
    "# Function to tokenize text using spaCy\n",
    "def tokenize_text(text):\n",
    "    # Process the text with the NLP object\n",
    "    doc = nlp(text)\n",
    "    # Create a list of tokenized words\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('C:/Users/xx/W_processed.csv')\n",
    "\n",
    "# Tokenize the 'processed_content' column\n",
    "df['tokenized_content'] = df['processed_content'].apply(tokenize_text)\n",
    "\n",
    "# Now, 'tokenized_content' contains the tokenized words as lists\n",
    "# If you need to save this DataFrame, consider converting the lists to strings\n",
    "df['tokenized_content_str'] = df['tokenized_content'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('W1_tokenized.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
